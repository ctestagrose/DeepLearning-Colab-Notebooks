{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "71212f203e4e454dafde573ded093923": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6349011411304757811f99808a2c49cf",
              "IPY_MODEL_17c47be29cb045aab14e851f910b736c",
              "IPY_MODEL_394f4e7dbbb64ef2918a04121de03451"
            ],
            "layout": "IPY_MODEL_b2df13cd0ad04a74a0715916a1905a4c"
          }
        },
        "6349011411304757811f99808a2c49cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b5a37e3060746f2adb401b24a740403",
            "placeholder": "​",
            "style": "IPY_MODEL_d573a455ceaa439d853f72c3a7f1b7cc",
            "value": "100%"
          }
        },
        "17c47be29cb045aab14e851f910b736c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f04795b2298497689cdc6190a98c056",
            "max": 417041973,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26213da075cf47daa71e24e8100affb1",
            "value": 417041973
          }
        },
        "394f4e7dbbb64ef2918a04121de03451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0951faf8a55f4c2bbe20b99eec07e767",
            "placeholder": "​",
            "style": "IPY_MODEL_a87f85927086480598c2835483ed2de7",
            "value": " 398M/398M [00:05&lt;00:00, 74.7MB/s]"
          }
        },
        "b2df13cd0ad04a74a0715916a1905a4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b5a37e3060746f2adb401b24a740403": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d573a455ceaa439d853f72c3a7f1b7cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f04795b2298497689cdc6190a98c056": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26213da075cf47daa71e24e8100affb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0951faf8a55f4c2bbe20b99eec07e767": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a87f85927086480598c2835483ed2de7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ball Classification \n",
        "\n",
        "###The Problem:\n",
        "Given an image of a ball (baseball, basketball, etc), provide the classification of that ball. \n",
        "\n",
        "###The Input:\n",
        "3 channel RGB image of a ball\n",
        "###The Output: \n",
        "Classification of that ball into one of 30 classes. \n",
        "\n",
        "\n",
        "This notebook with use the data found here: \n",
        "https://www.kaggle.com/datasets/gpiosenka/balls-image-classification\n",
        "\n",
        "The data was loaded as is to Google Drive and used to train, validate, and test the model used in this notebook.\n",
        "\n",
        "The code in this notebook was written by:\n",
        "Conrad Testagrose\n"
      ],
      "metadata": {
        "id": "793DteTLlBq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets install the necessary dependencies that I would like to use to accomplish\n",
        "this classification problem. \n",
        "\n",
        "PyTorch Pretrained ViT is a library written by Lukemelas that provides a pretrained Vision Transformers. ViTs provide exceptional accuracy but require an immense amoutn of data to train from scratch. Therefore we will use this library to implement a vision transformer. "
      ],
      "metadata": {
        "id": "cMMChP2vmSl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install monai==0.8.1\n",
        "!pip install pytorch_pretrained_vit "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCJVbf-JZBhd",
        "outputId": "35f7d42d-d997-4415-ab74-f1cbeaf6837e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting monai==0.8.1\n",
            "  Downloading monai-0.8.1-202202162213-py3-none-any.whl (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.9/721.9 KB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.9/dist-packages (from monai==0.8.1) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from monai==0.8.1) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->monai==0.8.1) (4.5.0)\n",
            "Installing collected packages: monai\n",
            "Successfully installed monai-0.8.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch_pretrained_vit in /usr/local/lib/python3.9/dist-packages (0.0.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_vit) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->pytorch_pretrained_vit) (4.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will import the required libraries/dependencies.\n"
      ],
      "metadata": {
        "id": "o_D8R7Zanllm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nK2S_Q5MUPuf"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "import torch \n",
        "import torchvision\n",
        "import pandas as pd\n",
        "from monai.data import Dataset\n",
        "import cv2\n",
        "import json\n",
        "import random\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "from monai.data import CacheDataset, DataLoader, Dataset\n",
        "from pytorch_pretrained_vit import ViT\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will tead the csv containing the filepaths and the labels. Each entry in the csv file will be added to corresponding key values in the data dictionary. \n",
        "\n",
        "We can also save this data dictionary as a JSON file if we needed to.\n",
        "\n",
        "We will print the first index of the \"Train\" key in the dictionary to visualize the structure"
      ],
      "metadata": {
        "id": "U7rxnU8hnsu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create data dictionary, this makes the process easier.\n",
        "\n",
        "data_dict = {\"Train\":[], \"Valid\":[], \"Test\":[]}\n",
        "labels = []\n",
        "for i in range(30):\n",
        "  temp = [0]*30\n",
        "  temp[i] = 1\n",
        "  labels.append(temp)\n",
        "\n",
        "data = pd.read_csv(\"./drive/MyDrive/Ball_Data/balls.csv\")\n",
        "\n",
        "for index, item in data.iterrows():\n",
        "  data_set = item[\"data set\"].capitalize()\n",
        "  temp_dict = {\"image\":\"./drive/MyDrive/Ball_Data/\"+item[\"filepaths\"],\n",
        "               \"label\": labels[int(item[\"class id\"])]}\n",
        "  data_dict[data_set].append(temp_dict)\n",
        "\n",
        "print(data_dict[\"Train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zXv5LwvWOLY",
        "outputId": "340ae965-014e-421a-ceee-fcc16929ded8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'image': './drive/MyDrive/Ball_Data/train/baseball/001.jpg', 'label': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now take the lists of dictionaries and shuffle them. We will create a list for the Train, Validation, and Test sets from the data dictionary that we created prior. \n",
        "\n",
        "Let's make sure to shuffle them so that the data is not in a learnable order. \n",
        "\n",
        "We will also visualize the image shapes. This will help dictate which basic transforms we use when creating our batches. If the image needs more channels or if the image needs to have its shape changes to meet the needs of the algorithm we are using. "
      ],
      "metadata": {
        "id": "YIwZIYwAvEe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get individual lists of data\n",
        "train_data = data_dict[\"Train\"]\n",
        "valid_data = data_dict[\"Valid\"]\n",
        "test_data = data_dict[\"Test\"]\n",
        "\n",
        "#Random shuffle the data\n",
        "random.Random(42).shuffle(train_data)\n",
        "random.Random(42).shuffle(valid_data)\n",
        "random.Random(42).shuffle(test_data)"
      ],
      "metadata": {
        "id": "JEHSosrDZl92"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(train_data[0][\"image\"])\n",
        "print(train_data[0][\"image\"], train_data[0][\"label\"])\n",
        "print(img.shape)\n",
        "\n",
        "img = cv2.imread(valid_data[0][\"image\"])\n",
        "print(valid_data[0][\"image\"], valid_data[0][\"label\"])\n",
        "print(img.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8JIl8NVcWL2",
        "outputId": "7219cbe6-cc7a-414f-d78a-766f8975f891"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./drive/MyDrive/Ball_Data/train/tennis ball/008.jpg [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "(224, 224, 3)\n",
            "./drive/MyDrive/Ball_Data/valid/football/4.jpg [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "(224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to create our dataloader. This will help us load the data to the model in the form of batches. "
      ],
      "metadata": {
        "id": "xjrqd-9Qu0FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the data loader used to load the data to the model. \n",
        "class dataloader(Dataset):\n",
        "    def __init__(self, dict, transforms):\n",
        "        self.dict = dict\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dict)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = cv2.imread(self.dict[index]['image'])\n",
        "        image = self.transforms(image)\n",
        "        label = self.dict[index]['label']\n",
        "        label = torch.FloatTensor(label)\n",
        "        return image, label\n",
        "\n",
        "class val_dataloader(Dataset):\n",
        "    def __init__(self, dict, transforms):\n",
        "        self.dict = dict\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dict)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = cv2.imread(self.dict[index]['image'])\n",
        "        image = self.transforms(image)\n",
        "        label = self.dict[index]['label']\n",
        "        label = torch.FloatTensor(label)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "1-YG46WzY5kU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we get to the fun part, the transforms. Typically for image classification the transforms will follow a general path:\n",
        "1. Load image\n",
        "2. Adjust the shape of the image if needed\n",
        "3. Resize\n",
        "4. Apply linear transforms or augmentations\n",
        "5. Make it a tensor\n",
        "\n",
        "For validation sets we would typically exclude the linear or augmentation transforms. \n",
        "\n",
        "In this code I have selected to AutoAugment the images. PyTorch includes this transform as a way to automatically augment the images in your dataset to a certain policy. I have selected the ImageNet policy. The following webpage provides a good overview of what data augmentation is:\n",
        "\n",
        "https://research.aimultiple.com/data-augmentation/\n",
        "\n",
        "The key takeaway is that it helps to synthetically \"create\" new data samples from our existing data to help diversify the data we train our model with. Without data augmentation we may be able to achieve high levels of accuracy quickly on images similar to our training set but we may perform poorly on real-world samples that are not visually similar to our training data. Data Augmentation will help to include more variety and hopefully increase the generalizablity of our algorithm. "
      ],
      "metadata": {
        "id": "y-gWrH8Vu9Iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "pytorch_Train = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Resize((int(299), int(299))),\n",
        "    T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),\n",
        "    T.ToTensor()]\n",
        ")\n",
        "\n",
        "pytorch_Validate = torchvision.transforms.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Resize((int(299), int(299))),\n",
        "    T.ToTensor()]\n",
        ")"
      ],
      "metadata": {
        "id": "vqX2CV2Ua5St"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create our model. Rather than create a model from scratch, I will use transfer learning to transfer the weights of a pretrained model to the model used in this project. \n",
        "\n",
        "Transfer learning helps us train the model and achieve higher levels of accuracy quickly. We transfer the weights of a model trained on a large and general dataset such as ImageNet or Microsoft COCO to our model. We add an additional layer to the end of this model which will help us finetune this model on our dataset of ball images. \n",
        "\n"
      ],
      "metadata": {
        "id": "wZG_nGhIxvJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create our model class\n",
        "class ModelDefinition():\n",
        "    def __init__(self, num_class: int, pretrained_flag=True, dropout_ratio=0.5, fc_nodes=1024, patch_size=32, img_size=int):\n",
        "        self.num_class = num_class\n",
        "        self.pretrain_flag = pretrained_flag\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.fc_nodes = fc_nodes\n",
        "        self.patch_size=patch_size\n",
        "        self.img_size=img_size\n",
        "\n",
        "    def inception_v3(self, device='cpu'):\n",
        "        model = models.inception_v3(pretrained=self.pretrain_flag)\n",
        "        model.fc = nn.Sequential(\n",
        "            nn.Linear(model.fc.in_features, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, self.num_class))\n",
        "        model.aux_logits = False\n",
        "        model.to(device)\n",
        "        return model\n",
        "\n",
        "    def ViT_Pretrained(self, device='cpu'):\n",
        "        if self.patch_size == 32:\n",
        "            model = ViT('B_32', pretrained=True, num_classes=self.num_class, image_size=self.img_size)\n",
        "        if self.patch_size == 16:\n",
        "            model = ViT('B_16', pretrained=True, num_classes=self.num_class, image_size=self.img_size)\n",
        "        model.aux_logits = False\n",
        "        model.to(device)\n",
        "        return model"
      ],
      "metadata": {
        "id": "m8XrwACDdryA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now set the parameters that we will use to train our model such as the learning rate, batch size, the number of classes, etc.\n",
        "\n",
        "We will also create our dataloader objects that we will use when training the model. "
      ],
      "metadata": {
        "id": "vf8ssFQJyyFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "num_classes = 30\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "dropout_ratio = 0.50\n",
        "patch_size = 32\n",
        "epochs = 100\n",
        "vit_patch_size = 32\n",
        "model_path = \"./drive/MyDrive/Ball_Data/models\"\n",
        "model_name = \"ViT_Pretrained\"\n",
        "\n",
        "train_ds = dataloader(train_data, transforms=pytorch_Train)\n",
        "validation_ds = val_dataloader(valid_data, transforms=pytorch_Validate)\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=int(batch_size), num_workers=8)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_ds, batch_size=int(batch_size), num_workers=8)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTMBc2bAeWkp",
        "outputId": "e7a72185-f51b-4d9f-ecd8-45f0ade1d21f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets initialize our model select a loss function and optimizer. From my experience, SGD provides better results for the ViT we will be using. If we use Inception V3, either will work well but Adam has provided me with great results in the past. "
      ],
      "metadata": {
        "id": "pTeQ79nRzHLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod = ModelDefinition(num_classes, pretrained_flag=\"pretrained\", img_size = 299)\n",
        "\n",
        "if model_name == 'InceptionV3':\n",
        "    model = mod.inception_v3()\n",
        "    #model = nn.DataParallel(model)\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
        "elif model_name == 'ViT_Pretrained':\n",
        "    model = mod.ViT_Pretrained()\n",
        "    #model = nn.DataParallel(model)\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum=0.9)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "71212f203e4e454dafde573ded093923",
            "6349011411304757811f99808a2c49cf",
            "17c47be29cb045aab14e851f910b736c",
            "394f4e7dbbb64ef2918a04121de03451",
            "b2df13cd0ad04a74a0715916a1905a4c",
            "6b5a37e3060746f2adb401b24a740403",
            "d573a455ceaa439d853f72c3a7f1b7cc",
            "8f04795b2298497689cdc6190a98c056",
            "26213da075cf47daa71e24e8100affb1",
            "0951faf8a55f4c2bbe20b99eec07e767",
            "a87f85927086480598c2835483ed2de7"
          ]
        },
        "id": "ckVNG-nvgFYl",
        "outputId": "6cc145d2-fee8-4443-f35e-fb9ea2fc47ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_32.pth\" to /root/.cache/torch/hub/checkpoints/B_32.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/398M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71212f203e4e454dafde573ded093923"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resized positional embeddings from torch.Size([1, 50, 768]) to torch.Size([1, 82, 768])\n",
            "Loaded pretrained weights.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
              "  (positional_embedding): PositionalEmbedding1D()\n",
              "  (transformer): Transformer(\n",
              "    (blocks): ModuleList(\n",
              "      (0): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (6): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (7): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (8): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (9): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (10): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (11): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  (fc): Linear(in_features=768, out_features=30, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we can train. We will train for 100 epochs and save the model based on its performance on the validation set. Let this run, it can take some time. \n",
        "\n",
        "Validation accuracy is a good sign that our model is training, however, saving the model based on it can result in us saving a model that overfits the data. We can use many different metrics to save the model though. Different metrics to monitor are:\n",
        "- Validation Loss\n",
        "- F1 Scores\n",
        "- Kappa\n",
        "\n",
        "We will use validation loss. Validation loss will decrease overtime if we are training correctly. It will eventually start to increase which is a sign the model is overfitting. We want to save when validation loss is at its lowest prior to increasing. \n",
        "\n",
        "This is not to say that we can not use the accuracy. It is best to determine the best metric to save by with some trial and error. "
      ],
      "metadata": {
        "id": "QmplOntqzc8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop\n",
        "best_metric = -1\n",
        "best_metric2 = 1000\n",
        "best_metric_epoch = -1\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "val_interval = 1\n",
        "\n",
        "print(\"Training...\")\n",
        "for epoch in range(epochs):\n",
        "    print(\"-\" * 10)\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    for batch_data in train_loader:\n",
        "        step += 1\n",
        "        inputs, labels = batch_data[0].to(device), batch_data[1].type(torch.float).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_len = len(train_ds) // train_loader.batch_size\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss: .4f}\")\n",
        "\n",
        "    if (epoch + 1) % int(val_interval) == 0:\n",
        "        model.eval()\n",
        "        num_correct = 0.0\n",
        "        metric_count = 0\n",
        "        val_epoch_loss = 0\n",
        "        val_step = 0\n",
        "        with torch.no_grad():\n",
        "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
        "            y = torch.tensor([], dtype=torch.long, device=device)\n",
        "            for val_data in validation_loader:\n",
        "                val_step += 1\n",
        "                val_images, val_labels = val_data[0].to(device), val_data[1].type(torch.float).to(device)\n",
        "                val_output = model(val_images)\n",
        "                value = torch.eq(val_output.argmax(dim=1), val_labels.argmax(dim=1))\n",
        "                val_loss = loss_function(val_output, val_labels)\n",
        "                val_epoch_loss += val_loss.item()\n",
        "                metric_count += len(value)\n",
        "                num_correct += value.sum().item()\n",
        "                val_epoch_len = len(validation_ds) // validation_loader.batch_size\n",
        "            metric = num_correct / metric_count\n",
        "            metric2 = val_epoch_loss\n",
        "            metric_values.append(metric)\n",
        "            if metric2 < best_metric2:\n",
        "                best_metric = metric\n",
        "                best_metric2 = metric2\n",
        "                best_metric_epoch = epoch + 1\n",
        "                torch.save(model.state_dict(), model_path + \"/best_model_vit.pth\")\n",
        "                print('Saved new model')\n",
        "            print(\"Current Epoch: {} current accuracy: {:.4f}\"\n",
        "                  \" Best accuracy: {:.4f} at epoch {}\".format(epoch + 1, metric, best_metric, best_metric_epoch))\n",
        "            print(\"validation_accuracy: \" + str(metric) + \" Epoch Number: \" + str(epoch + 1))\n",
        "            print(\"Validation Loss: \", val_epoch_loss)\n",
        "print(f\"training completed, best_metric: {best_metric: .4f}\"\n",
        "      f\" at epoch: {best_metric_epoch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyFBAPzxhorS",
        "outputId": "634420c4-6368-467e-a635-6356efbeaac1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "----------\n",
            "epoch 1 average loss:  3.3619\n",
            "Saved new model\n",
            "Current Epoch: 1 current accuracy: 0.5867 Best accuracy: 0.5867 at epoch 1\n",
            "validation_accuracy: 0.5866666666666667 Epoch Number: 1\n",
            "Validation Loss:  9.893285274505615\n",
            "----------\n",
            "epoch 2 average loss:  3.2489\n",
            "Saved new model\n",
            "Current Epoch: 2 current accuracy: 0.6267 Best accuracy: 0.6267 at epoch 2\n",
            "validation_accuracy: 0.6266666666666667 Epoch Number: 2\n",
            "Validation Loss:  9.50794506072998\n",
            "----------\n",
            "epoch 3 average loss:  3.1199\n",
            "Saved new model\n",
            "Current Epoch: 3 current accuracy: 0.6267 Best accuracy: 0.6267 at epoch 3\n",
            "validation_accuracy: 0.6266666666666667 Epoch Number: 3\n",
            "Validation Loss:  9.076879978179932\n",
            "----------\n",
            "epoch 4 average loss:  2.9752\n",
            "Saved new model\n",
            "Current Epoch: 4 current accuracy: 0.6267 Best accuracy: 0.6267 at epoch 4\n",
            "validation_accuracy: 0.6266666666666667 Epoch Number: 4\n",
            "Validation Loss:  8.599461317062378\n",
            "----------\n",
            "epoch 5 average loss:  2.8133\n",
            "Saved new model\n",
            "Current Epoch: 5 current accuracy: 0.6267 Best accuracy: 0.6267 at epoch 5\n",
            "validation_accuracy: 0.6266666666666667 Epoch Number: 5\n",
            "Validation Loss:  8.055155277252197\n",
            "----------\n",
            "epoch 6 average loss:  2.6270\n",
            "Saved new model\n",
            "Current Epoch: 6 current accuracy: 0.6533 Best accuracy: 0.6533 at epoch 6\n",
            "validation_accuracy: 0.6533333333333333 Epoch Number: 6\n",
            "Validation Loss:  7.4412267208099365\n",
            "----------\n",
            "epoch 7 average loss:  2.4326\n",
            "Saved new model\n",
            "Current Epoch: 7 current accuracy: 0.6600 Best accuracy: 0.6600 at epoch 7\n",
            "validation_accuracy: 0.66 Epoch Number: 7\n",
            "Validation Loss:  6.791967391967773\n",
            "----------\n",
            "epoch 8 average loss:  2.2131\n",
            "Saved new model\n",
            "Current Epoch: 8 current accuracy: 0.6867 Best accuracy: 0.6867 at epoch 8\n",
            "validation_accuracy: 0.6866666666666666 Epoch Number: 8\n",
            "Validation Loss:  6.1293076276779175\n",
            "----------\n",
            "epoch 9 average loss:  1.9909\n",
            "Saved new model\n",
            "Current Epoch: 9 current accuracy: 0.6933 Best accuracy: 0.6933 at epoch 9\n",
            "validation_accuracy: 0.6933333333333334 Epoch Number: 9\n",
            "Validation Loss:  5.468639016151428\n",
            "----------\n",
            "epoch 10 average loss:  1.7789\n",
            "Saved new model\n",
            "Current Epoch: 10 current accuracy: 0.7333 Best accuracy: 0.7333 at epoch 10\n",
            "validation_accuracy: 0.7333333333333333 Epoch Number: 10\n",
            "Validation Loss:  4.839839339256287\n",
            "----------\n",
            "epoch 11 average loss:  1.5862\n",
            "Saved new model\n",
            "Current Epoch: 11 current accuracy: 0.7533 Best accuracy: 0.7533 at epoch 11\n",
            "validation_accuracy: 0.7533333333333333 Epoch Number: 11\n",
            "Validation Loss:  4.275233149528503\n",
            "----------\n",
            "epoch 12 average loss:  1.3945\n",
            "Saved new model\n",
            "Current Epoch: 12 current accuracy: 0.7867 Best accuracy: 0.7867 at epoch 12\n",
            "validation_accuracy: 0.7866666666666666 Epoch Number: 12\n",
            "Validation Loss:  3.753042459487915\n",
            "----------\n",
            "epoch 13 average loss:  1.2340\n",
            "Saved new model\n",
            "Current Epoch: 13 current accuracy: 0.7867 Best accuracy: 0.7867 at epoch 13\n",
            "validation_accuracy: 0.7866666666666666 Epoch Number: 13\n",
            "Validation Loss:  3.3035253286361694\n",
            "----------\n",
            "epoch 14 average loss:  1.0890\n",
            "Saved new model\n",
            "Current Epoch: 14 current accuracy: 0.8200 Best accuracy: 0.8200 at epoch 14\n",
            "validation_accuracy: 0.82 Epoch Number: 14\n",
            "Validation Loss:  2.931868016719818\n",
            "----------\n",
            "epoch 15 average loss:  0.9714\n",
            "Saved new model\n",
            "Current Epoch: 15 current accuracy: 0.8400 Best accuracy: 0.8400 at epoch 15\n",
            "validation_accuracy: 0.84 Epoch Number: 15\n",
            "Validation Loss:  2.5900906920433044\n",
            "----------\n",
            "epoch 16 average loss:  0.8689\n",
            "Saved new model\n",
            "Current Epoch: 16 current accuracy: 0.8733 Best accuracy: 0.8733 at epoch 16\n",
            "validation_accuracy: 0.8733333333333333 Epoch Number: 16\n",
            "Validation Loss:  2.309055805206299\n",
            "----------\n",
            "epoch 17 average loss:  0.7743\n",
            "Saved new model\n",
            "Current Epoch: 17 current accuracy: 0.8800 Best accuracy: 0.8800 at epoch 17\n",
            "validation_accuracy: 0.88 Epoch Number: 17\n",
            "Validation Loss:  2.0608484745025635\n",
            "----------\n",
            "epoch 18 average loss:  0.6969\n",
            "Saved new model\n",
            "Current Epoch: 18 current accuracy: 0.8933 Best accuracy: 0.8933 at epoch 18\n",
            "validation_accuracy: 0.8933333333333333 Epoch Number: 18\n",
            "Validation Loss:  1.8433186411857605\n",
            "----------\n",
            "epoch 19 average loss:  0.6317\n",
            "Saved new model\n",
            "Current Epoch: 19 current accuracy: 0.9067 Best accuracy: 0.9067 at epoch 19\n",
            "validation_accuracy: 0.9066666666666666 Epoch Number: 19\n",
            "Validation Loss:  1.6632855534553528\n",
            "----------\n",
            "epoch 20 average loss:  0.5732\n",
            "Saved new model\n",
            "Current Epoch: 20 current accuracy: 0.9133 Best accuracy: 0.9133 at epoch 20\n",
            "validation_accuracy: 0.9133333333333333 Epoch Number: 20\n",
            "Validation Loss:  1.501164048910141\n",
            "----------\n",
            "epoch 21 average loss:  0.5311\n",
            "Saved new model\n",
            "Current Epoch: 21 current accuracy: 0.9133 Best accuracy: 0.9133 at epoch 21\n",
            "validation_accuracy: 0.9133333333333333 Epoch Number: 21\n",
            "Validation Loss:  1.367359310388565\n",
            "----------\n",
            "epoch 22 average loss:  0.4875\n",
            "Saved new model\n",
            "Current Epoch: 22 current accuracy: 0.9200 Best accuracy: 0.9200 at epoch 22\n",
            "validation_accuracy: 0.92 Epoch Number: 22\n",
            "Validation Loss:  1.233229160308838\n",
            "----------\n",
            "epoch 23 average loss:  0.4410\n",
            "Saved new model\n",
            "Current Epoch: 23 current accuracy: 0.9267 Best accuracy: 0.9267 at epoch 23\n",
            "validation_accuracy: 0.9266666666666666 Epoch Number: 23\n",
            "Validation Loss:  1.1247303485870361\n",
            "----------\n",
            "epoch 24 average loss:  0.4034\n",
            "Saved new model\n",
            "Current Epoch: 24 current accuracy: 0.9267 Best accuracy: 0.9267 at epoch 24\n",
            "validation_accuracy: 0.9266666666666666 Epoch Number: 24\n",
            "Validation Loss:  1.01480832695961\n",
            "----------\n",
            "epoch 25 average loss:  0.3782\n",
            "Saved new model\n",
            "Current Epoch: 25 current accuracy: 0.9333 Best accuracy: 0.9333 at epoch 25\n",
            "validation_accuracy: 0.9333333333333333 Epoch Number: 25\n",
            "Validation Loss:  0.9263912737369537\n",
            "----------\n",
            "epoch 26 average loss:  0.3563\n",
            "Saved new model\n",
            "Current Epoch: 26 current accuracy: 0.9333 Best accuracy: 0.9333 at epoch 26\n",
            "validation_accuracy: 0.9333333333333333 Epoch Number: 26\n",
            "Validation Loss:  0.8514818847179413\n",
            "----------\n",
            "epoch 27 average loss:  0.3356\n",
            "Saved new model\n",
            "Current Epoch: 27 current accuracy: 0.9533 Best accuracy: 0.9533 at epoch 27\n",
            "validation_accuracy: 0.9533333333333334 Epoch Number: 27\n",
            "Validation Loss:  0.777633011341095\n",
            "----------\n",
            "epoch 28 average loss:  0.2999\n",
            "Saved new model\n",
            "Current Epoch: 28 current accuracy: 0.9600 Best accuracy: 0.9600 at epoch 28\n",
            "validation_accuracy: 0.96 Epoch Number: 28\n",
            "Validation Loss:  0.7106348723173141\n",
            "----------\n",
            "epoch 29 average loss:  0.2943\n",
            "Saved new model\n",
            "Current Epoch: 29 current accuracy: 0.9733 Best accuracy: 0.9733 at epoch 29\n",
            "validation_accuracy: 0.9733333333333334 Epoch Number: 29\n",
            "Validation Loss:  0.6534547954797745\n",
            "----------\n",
            "epoch 30 average loss:  0.2698\n",
            "Saved new model\n",
            "Current Epoch: 30 current accuracy: 0.9733 Best accuracy: 0.9733 at epoch 30\n",
            "validation_accuracy: 0.9733333333333334 Epoch Number: 30\n",
            "Validation Loss:  0.5853659212589264\n",
            "----------\n",
            "epoch 31 average loss:  0.2487\n",
            "Saved new model\n",
            "Current Epoch: 31 current accuracy: 0.9733 Best accuracy: 0.9733 at epoch 31\n",
            "validation_accuracy: 0.9733333333333334 Epoch Number: 31\n",
            "Validation Loss:  0.5449181348085403\n",
            "----------\n",
            "epoch 32 average loss:  0.2449\n",
            "Saved new model\n",
            "Current Epoch: 32 current accuracy: 0.9800 Best accuracy: 0.9800 at epoch 32\n",
            "validation_accuracy: 0.98 Epoch Number: 32\n",
            "Validation Loss:  0.5181190967559814\n",
            "----------\n",
            "epoch 33 average loss:  0.2324\n",
            "Saved new model\n",
            "Current Epoch: 33 current accuracy: 0.9867 Best accuracy: 0.9867 at epoch 33\n",
            "validation_accuracy: 0.9866666666666667 Epoch Number: 33\n",
            "Validation Loss:  0.4689587950706482\n",
            "----------\n",
            "epoch 34 average loss:  0.2101\n",
            "Saved new model\n",
            "Current Epoch: 34 current accuracy: 0.9867 Best accuracy: 0.9867 at epoch 34\n",
            "validation_accuracy: 0.9866666666666667 Epoch Number: 34\n",
            "Validation Loss:  0.42979539930820465\n",
            "----------\n",
            "epoch 35 average loss:  0.1994\n",
            "Saved new model\n",
            "Current Epoch: 35 current accuracy: 0.9933 Best accuracy: 0.9933 at epoch 35\n",
            "validation_accuracy: 0.9933333333333333 Epoch Number: 35\n",
            "Validation Loss:  0.4005776271224022\n",
            "----------\n",
            "epoch 36 average loss:  0.1870\n",
            "Saved new model\n",
            "Current Epoch: 36 current accuracy: 0.9867 Best accuracy: 0.9867 at epoch 36\n",
            "validation_accuracy: 0.9866666666666667 Epoch Number: 36\n",
            "Validation Loss:  0.3723876029253006\n",
            "----------\n",
            "epoch 37 average loss:  0.1702\n",
            "Saved new model\n",
            "Current Epoch: 37 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 37\n",
            "validation_accuracy: 1.0 Epoch Number: 37\n",
            "Validation Loss:  0.343536414206028\n",
            "----------\n",
            "epoch 38 average loss:  0.1700\n",
            "Saved new model\n",
            "Current Epoch: 38 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 38\n",
            "validation_accuracy: 1.0 Epoch Number: 38\n",
            "Validation Loss:  0.32330868393182755\n",
            "----------\n",
            "epoch 39 average loss:  0.1555\n",
            "Saved new model\n",
            "Current Epoch: 39 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 39\n",
            "validation_accuracy: 1.0 Epoch Number: 39\n",
            "Validation Loss:  0.29933733493089676\n",
            "----------\n",
            "epoch 40 average loss:  0.1566\n",
            "Saved new model\n",
            "Current Epoch: 40 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 40\n",
            "validation_accuracy: 1.0 Epoch Number: 40\n",
            "Validation Loss:  0.2687917798757553\n",
            "----------\n",
            "epoch 41 average loss:  0.1539\n",
            "Saved new model\n",
            "Current Epoch: 41 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 41\n",
            "validation_accuracy: 1.0 Epoch Number: 41\n",
            "Validation Loss:  0.2563277520239353\n",
            "----------\n",
            "epoch 42 average loss:  0.1403\n",
            "Saved new model\n",
            "Current Epoch: 42 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 42\n",
            "validation_accuracy: 1.0 Epoch Number: 42\n",
            "Validation Loss:  0.23782486096024513\n",
            "----------\n",
            "epoch 43 average loss:  0.1373\n",
            "Saved new model\n",
            "Current Epoch: 43 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 43\n",
            "validation_accuracy: 1.0 Epoch Number: 43\n",
            "Validation Loss:  0.2211454100906849\n",
            "----------\n",
            "epoch 44 average loss:  0.1334\n",
            "Saved new model\n",
            "Current Epoch: 44 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 44\n",
            "validation_accuracy: 1.0 Epoch Number: 44\n",
            "Validation Loss:  0.20435598120093346\n",
            "----------\n",
            "epoch 45 average loss:  0.1254\n",
            "Saved new model\n",
            "Current Epoch: 45 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 45\n",
            "validation_accuracy: 1.0 Epoch Number: 45\n",
            "Validation Loss:  0.19071855768561363\n",
            "----------\n",
            "epoch 46 average loss:  0.1239\n",
            "Saved new model\n",
            "Current Epoch: 46 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 46\n",
            "validation_accuracy: 1.0 Epoch Number: 46\n",
            "Validation Loss:  0.18283336609601974\n",
            "----------\n",
            "epoch 47 average loss:  0.1156\n",
            "Saved new model\n",
            "Current Epoch: 47 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 47\n",
            "validation_accuracy: 1.0 Epoch Number: 47\n",
            "Validation Loss:  0.17804395407438278\n",
            "----------\n",
            "epoch 48 average loss:  0.1124\n",
            "Saved new model\n",
            "Current Epoch: 48 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 48\n",
            "validation_accuracy: 1.0 Epoch Number: 48\n",
            "Validation Loss:  0.157134760171175\n",
            "----------\n",
            "epoch 49 average loss:  0.1081\n",
            "Saved new model\n",
            "Current Epoch: 49 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 49\n",
            "validation_accuracy: 1.0 Epoch Number: 49\n",
            "Validation Loss:  0.14895128086209297\n",
            "----------\n",
            "epoch 50 average loss:  0.1082\n",
            "Current Epoch: 50 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 49\n",
            "validation_accuracy: 1.0 Epoch Number: 50\n",
            "Validation Loss:  0.14922168105840683\n",
            "----------\n",
            "epoch 51 average loss:  0.0952\n",
            "Saved new model\n",
            "Current Epoch: 51 current accuracy: 0.9933 Best accuracy: 0.9933 at epoch 51\n",
            "validation_accuracy: 0.9933333333333333 Epoch Number: 51\n",
            "Validation Loss:  0.14066876843571663\n",
            "----------\n",
            "epoch 52 average loss:  0.1031\n",
            "Saved new model\n",
            "Current Epoch: 52 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 52\n",
            "validation_accuracy: 1.0 Epoch Number: 52\n",
            "Validation Loss:  0.1279430016875267\n",
            "----------\n",
            "epoch 53 average loss:  0.0964\n",
            "Current Epoch: 53 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 52\n",
            "validation_accuracy: 1.0 Epoch Number: 53\n",
            "Validation Loss:  0.12989779189229012\n",
            "----------\n",
            "epoch 54 average loss:  0.0937\n",
            "Saved new model\n",
            "Current Epoch: 54 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 54\n",
            "validation_accuracy: 1.0 Epoch Number: 54\n",
            "Validation Loss:  0.11964185163378716\n",
            "----------\n",
            "epoch 55 average loss:  0.0825\n",
            "Saved new model\n",
            "Current Epoch: 55 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 55\n",
            "validation_accuracy: 1.0 Epoch Number: 55\n",
            "Validation Loss:  0.11646866984665394\n",
            "----------\n",
            "epoch 56 average loss:  0.0848\n",
            "Saved new model\n",
            "Current Epoch: 56 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 56\n",
            "validation_accuracy: 1.0 Epoch Number: 56\n",
            "Validation Loss:  0.11550270766019821\n",
            "----------\n",
            "epoch 57 average loss:  0.0789\n",
            "Saved new model\n",
            "Current Epoch: 57 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 57\n",
            "validation_accuracy: 1.0 Epoch Number: 57\n",
            "Validation Loss:  0.10355044528841972\n",
            "----------\n",
            "epoch 58 average loss:  0.0822\n",
            "Saved new model\n",
            "Current Epoch: 58 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 58\n",
            "validation_accuracy: 1.0 Epoch Number: 58\n",
            "Validation Loss:  0.09943760745227337\n",
            "----------\n",
            "epoch 59 average loss:  0.0831\n",
            "Saved new model\n",
            "Current Epoch: 59 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 59\n",
            "validation_accuracy: 1.0 Epoch Number: 59\n",
            "Validation Loss:  0.0971820019185543\n",
            "----------\n",
            "epoch 60 average loss:  0.0758\n",
            "Current Epoch: 60 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 59\n",
            "validation_accuracy: 1.0 Epoch Number: 60\n",
            "Validation Loss:  0.09833546541631222\n",
            "----------\n",
            "epoch 61 average loss:  0.0733\n",
            "Saved new model\n",
            "Current Epoch: 61 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 61\n",
            "validation_accuracy: 1.0 Epoch Number: 61\n",
            "Validation Loss:  0.09265962615609169\n",
            "----------\n",
            "epoch 62 average loss:  0.0702\n",
            "Current Epoch: 62 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 61\n",
            "validation_accuracy: 1.0 Epoch Number: 62\n",
            "Validation Loss:  0.0971999540925026\n",
            "----------\n",
            "epoch 63 average loss:  0.0729\n",
            "Saved new model\n",
            "Current Epoch: 63 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 63\n",
            "validation_accuracy: 1.0 Epoch Number: 63\n",
            "Validation Loss:  0.08977316878736019\n",
            "----------\n",
            "epoch 64 average loss:  0.0701\n",
            "Saved new model\n",
            "Current Epoch: 64 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 64\n",
            "validation_accuracy: 1.0 Epoch Number: 64\n",
            "Validation Loss:  0.08818764612078667\n",
            "----------\n",
            "epoch 65 average loss:  0.0776\n",
            "Saved new model\n",
            "Current Epoch: 65 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 65\n",
            "validation_accuracy: 1.0 Epoch Number: 65\n",
            "Validation Loss:  0.08051539957523346\n",
            "----------\n",
            "epoch 66 average loss:  0.0647\n",
            "Saved new model\n",
            "Current Epoch: 66 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 66\n",
            "validation_accuracy: 1.0 Epoch Number: 66\n",
            "Validation Loss:  0.07885822653770447\n",
            "----------\n",
            "epoch 67 average loss:  0.0584\n",
            "Current Epoch: 67 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 66\n",
            "validation_accuracy: 1.0 Epoch Number: 67\n",
            "Validation Loss:  0.08129689283668995\n",
            "----------\n",
            "epoch 68 average loss:  0.0707\n",
            "Saved new model\n",
            "Current Epoch: 68 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 68\n",
            "validation_accuracy: 1.0 Epoch Number: 68\n",
            "Validation Loss:  0.0748606026172638\n",
            "----------\n",
            "epoch 69 average loss:  0.0568\n",
            "Current Epoch: 69 current accuracy: 0.9933 Best accuracy: 1.0000 at epoch 68\n",
            "validation_accuracy: 0.9933333333333333 Epoch Number: 69\n",
            "Validation Loss:  0.07573305442929268\n",
            "----------\n",
            "epoch 70 average loss:  0.0543\n",
            "Saved new model\n",
            "Current Epoch: 70 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 70\n",
            "validation_accuracy: 1.0 Epoch Number: 70\n",
            "Validation Loss:  0.06870954763144255\n",
            "----------\n",
            "epoch 71 average loss:  0.0568\n",
            "Current Epoch: 71 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 70\n",
            "validation_accuracy: 1.0 Epoch Number: 71\n",
            "Validation Loss:  0.07042384706437588\n",
            "----------\n",
            "epoch 72 average loss:  0.0576\n",
            "Current Epoch: 72 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 70\n",
            "validation_accuracy: 1.0 Epoch Number: 72\n",
            "Validation Loss:  0.0687625976279378\n",
            "----------\n",
            "epoch 73 average loss:  0.0617\n",
            "Saved new model\n",
            "Current Epoch: 73 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 73\n",
            "validation_accuracy: 1.0 Epoch Number: 73\n",
            "Validation Loss:  0.06164991017431021\n",
            "----------\n",
            "epoch 74 average loss:  0.0600\n",
            "Saved new model\n",
            "Current Epoch: 74 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 74\n",
            "validation_accuracy: 1.0 Epoch Number: 74\n",
            "Validation Loss:  0.05706013459712267\n",
            "----------\n",
            "epoch 75 average loss:  0.0560\n",
            "Saved new model\n",
            "Current Epoch: 75 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 75\n",
            "validation_accuracy: 1.0 Epoch Number: 75\n",
            "Validation Loss:  0.05677373241633177\n",
            "----------\n",
            "epoch 76 average loss:  0.0552\n",
            "Current Epoch: 76 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 75\n",
            "validation_accuracy: 1.0 Epoch Number: 76\n",
            "Validation Loss:  0.058935726061463356\n",
            "----------\n",
            "epoch 77 average loss:  0.0520\n",
            "Current Epoch: 77 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 75\n",
            "validation_accuracy: 1.0 Epoch Number: 77\n",
            "Validation Loss:  0.05734763480722904\n",
            "----------\n",
            "epoch 78 average loss:  0.0512\n",
            "Saved new model\n",
            "Current Epoch: 78 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 78\n",
            "validation_accuracy: 1.0 Epoch Number: 78\n",
            "Validation Loss:  0.05273187905550003\n",
            "----------\n",
            "epoch 79 average loss:  0.0530\n",
            "Current Epoch: 79 current accuracy: 0.9933 Best accuracy: 1.0000 at epoch 78\n",
            "validation_accuracy: 0.9933333333333333 Epoch Number: 79\n",
            "Validation Loss:  0.06440605502575636\n",
            "----------\n",
            "epoch 80 average loss:  0.0446\n",
            "Current Epoch: 80 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 78\n",
            "validation_accuracy: 1.0 Epoch Number: 80\n",
            "Validation Loss:  0.056498133577406406\n",
            "----------\n",
            "epoch 81 average loss:  0.0499\n",
            "Current Epoch: 81 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 78\n",
            "validation_accuracy: 1.0 Epoch Number: 81\n",
            "Validation Loss:  0.06056779809296131\n",
            "----------\n",
            "epoch 82 average loss:  0.0442\n",
            "Current Epoch: 82 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 78\n",
            "validation_accuracy: 1.0 Epoch Number: 82\n",
            "Validation Loss:  0.05854035075753927\n",
            "----------\n",
            "epoch 83 average loss:  0.0482\n",
            "Current Epoch: 83 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 78\n",
            "validation_accuracy: 1.0 Epoch Number: 83\n",
            "Validation Loss:  0.06331733521074057\n",
            "----------\n",
            "epoch 84 average loss:  0.0463\n",
            "Current Epoch: 84 current accuracy: 0.9933 Best accuracy: 1.0000 at epoch 78\n",
            "validation_accuracy: 0.9933333333333333 Epoch Number: 84\n",
            "Validation Loss:  0.05829618964344263\n",
            "----------\n",
            "epoch 85 average loss:  0.0458\n",
            "Current Epoch: 85 current accuracy: 0.9933 Best accuracy: 1.0000 at epoch 78\n",
            "validation_accuracy: 0.9933333333333333 Epoch Number: 85\n",
            "Validation Loss:  0.05793406628072262\n",
            "----------\n",
            "epoch 86 average loss:  0.0519\n",
            "Saved new model\n",
            "Current Epoch: 86 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 86\n",
            "validation_accuracy: 1.0 Epoch Number: 86\n",
            "Validation Loss:  0.04923495464026928\n",
            "----------\n",
            "epoch 87 average loss:  0.0453\n",
            "Current Epoch: 87 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 86\n",
            "validation_accuracy: 1.0 Epoch Number: 87\n",
            "Validation Loss:  0.051840174943208694\n",
            "----------\n",
            "epoch 88 average loss:  0.0429\n",
            "Current Epoch: 88 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 86\n",
            "validation_accuracy: 1.0 Epoch Number: 88\n",
            "Validation Loss:  0.05205291789025068\n",
            "----------\n",
            "epoch 89 average loss:  0.0490\n",
            "Saved new model\n",
            "Current Epoch: 89 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 89\n",
            "validation_accuracy: 1.0 Epoch Number: 89\n",
            "Validation Loss:  0.04838563408702612\n",
            "----------\n",
            "epoch 90 average loss:  0.0341\n",
            "Saved new model\n",
            "Current Epoch: 90 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 90\n",
            "validation_accuracy: 1.0 Epoch Number: 90\n",
            "Validation Loss:  0.048228719271719456\n",
            "----------\n",
            "epoch 91 average loss:  0.0336\n",
            "Saved new model\n",
            "Current Epoch: 91 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 91\n",
            "validation_accuracy: 1.0 Epoch Number: 91\n",
            "Validation Loss:  0.04638811573386192\n",
            "----------\n",
            "epoch 92 average loss:  0.0346\n",
            "Saved new model\n",
            "Current Epoch: 92 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 92\n",
            "validation_accuracy: 1.0 Epoch Number: 92\n",
            "Validation Loss:  0.042108084075152874\n",
            "----------\n",
            "epoch 93 average loss:  0.0340\n",
            "Saved new model\n",
            "Current Epoch: 93 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 93\n",
            "validation_accuracy: 1.0 Epoch Number: 93\n",
            "Validation Loss:  0.04039432853460312\n",
            "----------\n",
            "epoch 94 average loss:  0.0407\n",
            "Current Epoch: 94 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 93\n",
            "validation_accuracy: 1.0 Epoch Number: 94\n",
            "Validation Loss:  0.04714991804212332\n",
            "----------\n",
            "epoch 95 average loss:  0.0314\n",
            "Saved new model\n",
            "Current Epoch: 95 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 95\n",
            "validation_accuracy: 1.0 Epoch Number: 95\n",
            "Validation Loss:  0.04038103902712464\n",
            "----------\n",
            "epoch 96 average loss:  0.0343\n",
            "Current Epoch: 96 current accuracy: 0.9933 Best accuracy: 1.0000 at epoch 95\n",
            "validation_accuracy: 0.9933333333333333 Epoch Number: 96\n",
            "Validation Loss:  0.04792628809809685\n",
            "----------\n",
            "epoch 97 average loss:  0.0280\n",
            "Saved new model\n",
            "Current Epoch: 97 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 97\n",
            "validation_accuracy: 1.0 Epoch Number: 97\n",
            "Validation Loss:  0.03649828489869833\n",
            "----------\n",
            "epoch 98 average loss:  0.0339\n",
            "Current Epoch: 98 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 97\n",
            "validation_accuracy: 1.0 Epoch Number: 98\n",
            "Validation Loss:  0.03844895865768194\n",
            "----------\n",
            "epoch 99 average loss:  0.0329\n",
            "Current Epoch: 99 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 97\n",
            "validation_accuracy: 1.0 Epoch Number: 99\n",
            "Validation Loss:  0.03920426219701767\n",
            "----------\n",
            "epoch 100 average loss:  0.0352\n",
            "Current Epoch: 100 current accuracy: 1.0000 Best accuracy: 1.0000 at epoch 97\n",
            "validation_accuracy: 1.0 Epoch Number: 100\n",
            "Validation Loss:  0.041201960295438766\n",
            "training completed, best_metric:  1.0000 at epoch: 97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will evaluate the performance of our algorithm on the test set. "
      ],
      "metadata": {
        "id": "T1I__y6A_hM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import sklearn.metrics as metrics\n",
        "from itertools import cycle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    Activations,\n",
        "    EnsureType)\n",
        "import progressbar\n",
        "\n",
        "test_data = data_dict[\"Test\"]\n",
        "\n",
        "random.Random(42).shuffle(test_data)\n",
        "\n",
        "pytorch_Test = torchvision.transforms.Compose([\n",
        "  torchvision.transforms.ToPILImage(),\n",
        "  torchvision.transforms.Resize((int(299), int(299))),\n",
        "  torchvision.transforms.ToTensor()]\n",
        ")\n",
        "\n",
        "test_ds = dataloader(test_data, transforms=pytorch_Test)\n",
        "test_loader = torch.utils.data.DataLoader(test_ds, num_workers=4)\n",
        "\n",
        "model_filename = model_path + '/best_model_vit.pth'\n",
        "model = mod.ViT_Pretrained()\n",
        "model.to(device)\n",
        "model.load_state_dict(torch.load(model_filename))\n",
        "model.eval()\n",
        "\n",
        "y_pred_trans = Compose([EnsureType(), Activations(sigmoid=True)])\n",
        "\n",
        "count = 0\n",
        "Image_FN_LIST = []\n",
        "Y_PRED_NP = np.zeros([len(test_data), num_classes])\n",
        "y_predicted = []\n",
        "y_true = []\n",
        "y_test = []\n",
        "y_score = []\n",
        "GT_LIST = []\n",
        "\n",
        "pb = progressbar.ProgressBar(maxval=len(test_data)+1)\n",
        "\n",
        "pb.start()\n",
        "with torch.no_grad():\n",
        "  for _testImage in test_loader:\n",
        "    image = torch.as_tensor(_testImage[0]).to(device)\n",
        "    y_test.append(np.array(_testImage[1]))\n",
        "    gt = _testImage[1].cpu().detach().numpy()\n",
        "    GT_LIST.append(int(np.argmax(gt[0])))\n",
        "    y_true.append(int(np.argmax(gt[0])))\n",
        "    y = model(image)\n",
        "    y_pred = y_pred_trans(y).to('cpu')\n",
        "    y_predicted.append(torch.argmax(y_pred).item())\n",
        "    pb.update(count)\n",
        "    count += 1 \n",
        "\n",
        "y_test = np.array([t.ravel() for t in y_test])\n",
        "y_score = np.array([t.ravel() for t in Y_PRED_NP]) \n",
        "\n",
        "print(classification_report(y_true, y_predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj04XIei2Z4p",
        "outputId": "1f5ada9a-e68e-40f6-f2ef-44f6021ba00f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resized positional embeddings from torch.Size([1, 50, 768]) to torch.Size([1, 82, 768])\n",
            "Loaded pretrained weights.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98% (148 of 151) |##################### | Elapsed Time: 0:00:09 ETA:   0:00:00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         5\n",
            "           1       0.83      1.00      0.91         5\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         5\n",
            "           4       1.00      1.00      1.00         5\n",
            "           5       1.00      1.00      1.00         5\n",
            "           6       1.00      1.00      1.00         5\n",
            "           7       1.00      1.00      1.00         5\n",
            "           8       0.83      1.00      0.91         5\n",
            "           9       1.00      1.00      1.00         5\n",
            "          10       1.00      1.00      1.00         5\n",
            "          11       1.00      1.00      1.00         5\n",
            "          12       1.00      1.00      1.00         5\n",
            "          13       1.00      1.00      1.00         5\n",
            "          14       0.83      1.00      0.91         5\n",
            "          15       1.00      0.80      0.89         5\n",
            "          16       1.00      1.00      1.00         5\n",
            "          17       1.00      0.80      0.89         5\n",
            "          18       1.00      1.00      1.00         5\n",
            "          19       1.00      1.00      1.00         5\n",
            "          20       1.00      1.00      1.00         5\n",
            "          21       1.00      1.00      1.00         5\n",
            "          22       1.00      1.00      1.00         5\n",
            "          23       1.00      1.00      1.00         5\n",
            "          24       1.00      1.00      1.00         5\n",
            "          25       1.00      1.00      1.00         5\n",
            "          26       1.00      1.00      1.00         5\n",
            "          27       1.00      0.80      0.89         5\n",
            "          28       1.00      1.00      1.00         5\n",
            "          29       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.98       150\n",
            "   macro avg       0.98      0.98      0.98       150\n",
            "weighted avg       0.98      0.98      0.98       150\n",
            "\n"
          ]
        }
      ]
    }
  ]
}