{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ball Classification \n",
        "\n",
        "###The Problem:\n",
        "Given an image of a ball (baseball, basketball, etc), provide the classification of that ball. \n",
        "\n",
        "###The Input:\n",
        "3 channel RGB image of a ball\n",
        "###The Output: \n",
        "Classification of that ball into one of 30 classes. \n",
        "\n",
        "\n",
        "This notebook with use the data found here: \n",
        "https://www.kaggle.com/datasets/gpiosenka/balls-image-classification\n",
        "\n",
        "The data was loaded as is to Google Drive and used to train, validate, and test the model used in this notebook.\n",
        "\n",
        "The code in this notebook was written by:\n",
        "Conrad Testagrose\n"
      ],
      "metadata": {
        "id": "793DteTLlBq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets install the necessary dependencies that I would like to use to accomplish\n",
        "this classification problem. \n",
        "\n",
        "PyTorch Pretrained ViT is a library written by Lukemelas that provides a pretrained Vision Transformers. ViTs provide exceptional accuracy but require an immense amoutn of data to train from scratch. Therefore we will use this library to implement a vision transformer. "
      ],
      "metadata": {
        "id": "cMMChP2vmSl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_pretrained_vit "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCJVbf-JZBhd",
        "outputId": "339baaaf-236e-4a6a-d324-547d8d5d6dff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch_pretrained_vit in /usr/local/lib/python3.9/dist-packages (0.0.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_vit) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->pytorch_pretrained_vit) (4.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will import the required libraries/dependencies.\n"
      ],
      "metadata": {
        "id": "o_D8R7Zanllm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nK2S_Q5MUPuf"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "import torch \n",
        "import torchvision\n",
        "import pandas as pd\n",
        "from monai.data import Dataset\n",
        "import cv2\n",
        "import json\n",
        "import random\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "from monai.data import CacheDataset, DataLoader, Dataset\n",
        "from pytorch_pretrained_vit import ViT\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will tead the csv containing the filepaths and the labels. Each entry in the csv file will be added to corresponding key values in the data dictionary. \n",
        "\n",
        "We can also save this data dictionary as a JSON file if we needed to.\n",
        "\n",
        "We will print the first index of the \"Train\" key in the dictionary to visualize the structure"
      ],
      "metadata": {
        "id": "U7rxnU8hnsu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create data dictionary, this makes the process easier.\n",
        "\n",
        "data_dict = {\"Train\":[], \"Valid\":[], \"Test\":[]}\n",
        "labels = []\n",
        "for i in range(30):\n",
        "  temp = [0]*30\n",
        "  temp[i] = 1\n",
        "  labels.append(temp)\n",
        "\n",
        "data = pd.read_csv(\"./drive/MyDrive/Ball_Data/balls.csv\")\n",
        "\n",
        "for index, item in data.iterrows():\n",
        "  data_set = item[\"data set\"].capitalize()\n",
        "  temp_dict = {\"image\":\"./drive/MyDrive/Ball_Data/\"+item[\"filepaths\"],\n",
        "               \"label\": labels[int(item[\"class id\"])]}\n",
        "  data_dict[data_set].append(temp_dict)\n",
        "\n",
        "print(data_dict[\"Train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zXv5LwvWOLY",
        "outputId": "1e0aea3e-9c53-47b0-8ac2-0463db91ac12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'image': './drive/MyDrive/Ball_Data/train/baseball/001.jpg', 'label': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now take the lists of dictionaries and shuffle them. We will create a list for the Train, Validation, and Test sets from the data dictionary that we created prior. \n",
        "\n",
        "Let's make sure to shuffle them so that the data is not in a learnable order. \n",
        "\n",
        "We will also visualize the image shapes. This will help dictate which basic transforms we use when creating our batches. If the image needs more channels or if the image needs to have its shape changes to meet the needs of the algorithm we are using. "
      ],
      "metadata": {
        "id": "YIwZIYwAvEe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get individual lists of data\n",
        "train_data = data_dict[\"Train\"]\n",
        "valid_data = data_dict[\"Valid\"]\n",
        "test_data = data_dict[\"Test\"]\n",
        "\n",
        "#Random shuffle the data\n",
        "random.Random(42).shuffle(train_data)\n",
        "random.Random(42).shuffle(valid_data)\n",
        "random.Random(42).shuffle(test_data)"
      ],
      "metadata": {
        "id": "JEHSosrDZl92"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(train_data[0][\"image\"])\n",
        "print(train_data[0][\"image\"], train_data[0][\"label\"])\n",
        "print(img.shape)\n",
        "\n",
        "img = cv2.imread(valid_data[0][\"image\"])\n",
        "print(valid_data[0][\"image\"], valid_data[0][\"label\"])\n",
        "print(img.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8JIl8NVcWL2",
        "outputId": "46ac69e3-a99b-47d3-c59a-b812b8226244"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./drive/MyDrive/Ball_Data/train/tennis ball/008.jpg [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "(224, 224, 3)\n",
            "./drive/MyDrive/Ball_Data/valid/football/4.jpg [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "(224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to create our dataloader. This will help us load the data to the model in the form of batches. "
      ],
      "metadata": {
        "id": "xjrqd-9Qu0FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the data loader used to load the data to the model. \n",
        "class dataloader(Dataset):\n",
        "    def __init__(self, dict, transforms):\n",
        "        self.dict = dict\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dict)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = cv2.imread(self.dict[index]['image'])\n",
        "        image = self.transforms(image)\n",
        "        label = self.dict[index]['label']\n",
        "        label = torch.FloatTensor(label)\n",
        "        return image, label\n",
        "\n",
        "class val_dataloader(Dataset):\n",
        "    def __init__(self, dict, transforms):\n",
        "        self.dict = dict\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dict)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = cv2.imread(self.dict[index]['image'])\n",
        "        image = self.transforms(image)\n",
        "        label = self.dict[index]['label']\n",
        "        label = torch.FloatTensor(label)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "1-YG46WzY5kU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we get to the fun part, the transforms. Typically for image classification the transforms will follow a general path:\n",
        "1. Load image\n",
        "2. Adjust the shape of the image if needed\n",
        "3. Resize\n",
        "4. Apply linear transforms or augmentations\n",
        "5. Make it a tensor\n",
        "\n",
        "For validation sets we would typically exclude the linear or augmentation transforms. \n",
        "\n",
        "In this code I have selected to AutoAugment the images. PyTorch includes this transform as a way to automatically augment the images in your dataset to a certain policy. I have selected the ImageNet policy. The following webpage provides a good overview of what data augmentation is:\n",
        "\n",
        "https://research.aimultiple.com/data-augmentation/\n",
        "\n",
        "The key takeaway is that it helps to synthetically \"create\" new data samples from our existing data to help diversify the data we train our model with. Without data augmentation we may be able to achieve high levels of accuracy quickly on images similar to our training set but we may perform poorly on real-world samples that are not visually similar to our training data. Data Augmentation will help to include more variety and hopefully increase the generalizablity of our algorithm. "
      ],
      "metadata": {
        "id": "y-gWrH8Vu9Iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "pytorch_Train = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Resize((int(299), int(299))),\n",
        "    T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),\n",
        "    T.ToTensor()]\n",
        ")\n",
        "\n",
        "pytorch_Validate = torchvision.transforms.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Resize((int(299), int(299))),\n",
        "    T.ToTensor()]\n",
        ")"
      ],
      "metadata": {
        "id": "vqX2CV2Ua5St"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create our model. Rather than create a model from scratch, I will use transfer learning to transfer the weights of a pretrained model to the model used in this project. \n",
        "\n",
        "Transfer learning helps us train the model and achieve higher levels of accuracy quickly. We transfer the weights of a model trained on a large and general dataset such as ImageNet or Microsoft COCO to our model. We add an additional layer to the end of this model which will help us finetune this model on our dataset of ball images. \n",
        "\n"
      ],
      "metadata": {
        "id": "wZG_nGhIxvJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create our model class\n",
        "class ModelDefinition():\n",
        "    def __init__(self, num_class: int, pretrained_flag=True, dropout_ratio=0.5, fc_nodes=1024, patch_size=32, img_size=int):\n",
        "        self.num_class = num_class\n",
        "        self.pretrain_flag = pretrained_flag\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.fc_nodes = fc_nodes\n",
        "        self.patch_size=patch_size\n",
        "        self.img_size=img_size\n",
        "\n",
        "    def inception_v3(self, device='cpu'):\n",
        "        model = models.inception_v3(pretrained=self.pretrain_flag)\n",
        "        model.fc = nn.Sequential(\n",
        "            nn.Linear(model.fc.in_features, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, self.num_class))\n",
        "        model.aux_logits = False\n",
        "        model.to(device)\n",
        "        return model\n",
        "\n",
        "    def ViT_Pretrained(self, device='cpu'):\n",
        "        if self.patch_size == 32:\n",
        "            model = ViT('B_32', pretrained=True, num_classes=self.num_class, image_size=self.img_size)\n",
        "        if self.patch_size == 16:\n",
        "            model = ViT('B_16', pretrained=True, num_classes=self.num_class, image_size=self.img_size)\n",
        "        model.aux_logits = False\n",
        "        model.to(device)\n",
        "        return model"
      ],
      "metadata": {
        "id": "m8XrwACDdryA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now set the parameters that we will use to train our model such as the learning rate, batch size, the number of classes, etc.\n",
        "\n",
        "We will also create our dataloader objects that we will use when training the model. "
      ],
      "metadata": {
        "id": "vf8ssFQJyyFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "num_classes = 30\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "dropout_ratio = 0.50\n",
        "patch_size = 32\n",
        "epochs = 100\n",
        "vit_patch_size = 32\n",
        "model_path = \"./drive/MyDrive/Ball_Data/models\"\n",
        "model_name = \"ViT_Pretrained\"\n",
        "\n",
        "train_ds = dataloader(train_data, transforms=pytorch_Train)\n",
        "validation_ds = val_dataloader(valid_data, transforms=pytorch_Validate)\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=int(batch_size), num_workers=8)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_ds, batch_size=int(batch_size), num_workers=8)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTMBc2bAeWkp",
        "outputId": "f81c7f8f-e320-44ae-b251-dc50db38f5da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets initialize our model select a loss function and optimizer. From my experience, SGD provides better results for the ViT we will be using. If we use Inception V3, either will work well but Adam has provided me with great results in the past. "
      ],
      "metadata": {
        "id": "pTeQ79nRzHLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod = ModelDefinition(num_classes, pretrained_flag=\"pretrained\", img_size = 299)\n",
        "\n",
        "if model_name == 'InceptionV3':\n",
        "    model = mod.inception_v3()\n",
        "    #model = nn.DataParallel(model)\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
        "elif model_name == 'ViT_Pretrained':\n",
        "    model = mod.ViT_Pretrained()\n",
        "    #model = nn.DataParallel(model)\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum=0.9)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckVNG-nvgFYl",
        "outputId": "36e391fd-1b62-4ae0-ac42-984d0c6354ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resized positional embeddings from torch.Size([1, 50, 768]) to torch.Size([1, 82, 768])\n",
            "Loaded pretrained weights.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
              "  (positional_embedding): PositionalEmbedding1D()\n",
              "  (transformer): Transformer(\n",
              "    (blocks): ModuleList(\n",
              "      (0): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (6): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (7): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (8): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (9): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (10): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (11): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  (fc): Linear(in_features=768, out_features=30, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we can train. We will train for 100 epochs and save the model based on its performance on the validation set. Let this run, it can take some time. \n",
        "\n",
        "Validation accuracy is a good sign that our model is training, however, saving the model based on it can result in us saving a model that overfits the data. We can use many different metrics to save the model though. Different metrics to monitor are:\n",
        "- Validation Loss\n",
        "- F1 Scores\n",
        "- Kappa\n",
        "\n",
        "We will use validation loss. Validation loss will decrease overtime if we are training correctly. It will eventually start to increase which is a sign the model is overfitting. We want to save when validation loss is at its lowest prior to increasing. \n",
        "\n",
        "This is not to say that we can not use the accuracy. It is best to determine the best metric to save by with some trial and error. "
      ],
      "metadata": {
        "id": "QmplOntqzc8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop\n",
        "best_metric = -1\n",
        "best_metric2 = 1000\n",
        "best_metric_epoch = -1\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "val_interval = 1\n",
        "\n",
        "print(\"Training...\")\n",
        "for epoch in range(epochs):\n",
        "    print(\"-\" * 10)\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    for batch_data in train_loader:\n",
        "        step += 1\n",
        "        inputs, labels = batch_data[0].to(device), batch_data[1].type(torch.float).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_len = len(train_ds) // train_loader.batch_size\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss: .4f}\")\n",
        "\n",
        "    if (epoch + 1) % int(val_interval) == 0:\n",
        "        model.eval()\n",
        "        num_correct = 0.0\n",
        "        metric_count = 0\n",
        "        val_epoch_loss = 0\n",
        "        val_step = 0\n",
        "        with torch.no_grad():\n",
        "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
        "            y = torch.tensor([], dtype=torch.long, device=device)\n",
        "            for val_data in validation_loader:\n",
        "                val_step += 1\n",
        "                val_images, val_labels = val_data[0].to(device), val_data[1].type(torch.float).to(device)\n",
        "                val_output = model(val_images)\n",
        "                value = torch.eq(val_output.argmax(dim=1), val_labels.argmax(dim=1))\n",
        "                val_loss = loss_function(val_output, val_labels)\n",
        "                val_epoch_loss += val_loss.item()\n",
        "                metric_count += len(value)\n",
        "                num_correct += value.sum().item()\n",
        "                val_epoch_len = len(validation_ds) // validation_loader.batch_size\n",
        "            metric = num_correct / metric_count\n",
        "            metric2 = val_epoch_loss\n",
        "            metric_values.append(metric)\n",
        "            if metric2 < best_metric2:\n",
        "                best_metric = metric\n",
        "                best_metric2 = metric2\n",
        "                best_metric_epoch = epoch + 1\n",
        "                torch.save(model.state_dict(), model_path + \"/best_model_vit.pth\")\n",
        "                print('Saved new model')\n",
        "            print(\"Current Epoch: {} current accuracy: {:.4f}\"\n",
        "                  \" Best accuracy: {:.4f} at epoch {}\".format(epoch + 1, metric, best_metric, best_metric_epoch))\n",
        "            print(\"validation_accuracy: \" + str(metric) + \" Epoch Number: \" + str(epoch + 1))\n",
        "            print(\"Validation Loss: \", val_epoch_loss)\n",
        "print(f\"training completed, best_metric: {best_metric: .4f}\"\n",
        "      f\" at epoch: {best_metric_epoch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyFBAPzxhorS",
        "outputId": "a2b2024f-b338-4e67-fd30-35a6b1ec83b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 average loss:  3.1435\n",
            "Saved new model\n",
            "Current Epoch: 1 current accuracy: 0.6133 Best accuracy: 0.6133 at epoch 1\n",
            "validation_accuracy: 0.6133333333333333 Epoch Number: 1\n",
            "Validation Loss:  9.145339965820312\n",
            "----------\n",
            "epoch 2 average loss:  2.9980\n",
            "Saved new model\n",
            "Current Epoch: 2 current accuracy: 0.6067 Best accuracy: 0.6067 at epoch 2\n",
            "validation_accuracy: 0.6066666666666667 Epoch Number: 2\n",
            "Validation Loss:  8.670431137084961\n",
            "----------\n",
            "epoch 3 average loss:  2.8363\n",
            "Saved new model\n",
            "Current Epoch: 3 current accuracy: 0.6333 Best accuracy: 0.6333 at epoch 3\n",
            "validation_accuracy: 0.6333333333333333 Epoch Number: 3\n",
            "Validation Loss:  8.133087873458862\n",
            "----------\n",
            "epoch 4 average loss:  2.6534\n",
            "Saved new model\n",
            "Current Epoch: 4 current accuracy: 0.6467 Best accuracy: 0.6467 at epoch 4\n",
            "validation_accuracy: 0.6466666666666666 Epoch Number: 4\n",
            "Validation Loss:  7.540449857711792\n",
            "----------\n",
            "epoch 5 average loss:  2.4555\n",
            "Saved new model\n",
            "Current Epoch: 5 current accuracy: 0.6600 Best accuracy: 0.6600 at epoch 5\n",
            "validation_accuracy: 0.66 Epoch Number: 5\n",
            "Validation Loss:  6.897287368774414\n",
            "----------\n",
            "epoch 6 average loss:  2.2401\n",
            "Saved new model\n",
            "Current Epoch: 6 current accuracy: 0.6867 Best accuracy: 0.6867 at epoch 6\n",
            "validation_accuracy: 0.6866666666666666 Epoch Number: 6\n",
            "Validation Loss:  6.228035569190979\n",
            "----------\n",
            "epoch 7 average loss:  2.0290\n",
            "Saved new model\n",
            "Current Epoch: 7 current accuracy: 0.7200 Best accuracy: 0.7200 at epoch 7\n",
            "validation_accuracy: 0.72 Epoch Number: 7\n",
            "Validation Loss:  5.55260956287384\n",
            "----------\n",
            "epoch 8 average loss:  1.8137\n",
            "Saved new model\n",
            "Current Epoch: 8 current accuracy: 0.7333 Best accuracy: 0.7333 at epoch 8\n",
            "validation_accuracy: 0.7333333333333333 Epoch Number: 8\n",
            "Validation Loss:  4.932283997535706\n",
            "----------\n",
            "epoch 9 average loss:  1.6136\n",
            "Saved new model\n",
            "Current Epoch: 9 current accuracy: 0.7533 Best accuracy: 0.7533 at epoch 9\n",
            "validation_accuracy: 0.7533333333333333 Epoch Number: 9\n",
            "Validation Loss:  4.348188877105713\n",
            "----------\n",
            "epoch 10 average loss:  1.4250\n",
            "Saved new model\n",
            "Current Epoch: 10 current accuracy: 0.7667 Best accuracy: 0.7667 at epoch 10\n",
            "validation_accuracy: 0.7666666666666667 Epoch Number: 10\n",
            "Validation Loss:  3.831769108772278\n",
            "----------\n",
            "epoch 11 average loss:  1.2540\n",
            "Saved new model\n",
            "Current Epoch: 11 current accuracy: 0.7933 Best accuracy: 0.7933 at epoch 11\n",
            "validation_accuracy: 0.7933333333333333 Epoch Number: 11\n",
            "Validation Loss:  3.3616929054260254\n",
            "----------\n",
            "epoch 12 average loss:  1.1172\n",
            "Saved new model\n",
            "Current Epoch: 12 current accuracy: 0.8000 Best accuracy: 0.8000 at epoch 12\n",
            "validation_accuracy: 0.8 Epoch Number: 12\n",
            "Validation Loss:  2.9734021425247192\n",
            "----------\n",
            "epoch 13 average loss:  0.9916\n",
            "Saved new model\n",
            "Current Epoch: 13 current accuracy: 0.8400 Best accuracy: 0.8400 at epoch 13\n",
            "validation_accuracy: 0.84 Epoch Number: 13\n",
            "Validation Loss:  2.62980580329895\n",
            "----------\n",
            "epoch 14 average loss:  0.8776\n",
            "Saved new model\n",
            "Current Epoch: 14 current accuracy: 0.8467 Best accuracy: 0.8467 at epoch 14\n",
            "validation_accuracy: 0.8466666666666667 Epoch Number: 14\n",
            "Validation Loss:  2.353977143764496\n",
            "----------\n",
            "epoch 15 average loss:  0.7873\n",
            "Saved new model\n",
            "Current Epoch: 15 current accuracy: 0.8733 Best accuracy: 0.8733 at epoch 15\n",
            "validation_accuracy: 0.8733333333333333 Epoch Number: 15\n",
            "Validation Loss:  2.0970948338508606\n",
            "----------\n",
            "epoch 16 average loss:  0.7224\n",
            "Saved new model\n",
            "Current Epoch: 16 current accuracy: 0.9000 Best accuracy: 0.9000 at epoch 16\n",
            "validation_accuracy: 0.9 Epoch Number: 16\n",
            "Validation Loss:  1.8842487335205078\n",
            "----------\n",
            "epoch 17 average loss:  0.6621\n",
            "Saved new model\n",
            "Current Epoch: 17 current accuracy: 0.9067 Best accuracy: 0.9067 at epoch 17\n",
            "validation_accuracy: 0.9066666666666666 Epoch Number: 17\n",
            "Validation Loss:  1.695500373840332\n",
            "----------\n",
            "epoch 18 average loss:  0.5832\n",
            "Saved new model\n",
            "Current Epoch: 18 current accuracy: 0.9067 Best accuracy: 0.9067 at epoch 18\n",
            "validation_accuracy: 0.9066666666666666 Epoch Number: 18\n",
            "Validation Loss:  1.5314126908779144\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will evaluate the performance of our algorithm on the test set. "
      ],
      "metadata": {
        "id": "T1I__y6A_hM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import sklearn.metrics as metrics\n",
        "from itertools import cycle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    Activations,\n",
        "    EnsureType)\n",
        "import progressbar\n",
        "\n",
        "test_data = data_dict[\"Test\"]\n",
        "\n",
        "random.Random(42).shuffle(test_data)\n",
        "\n",
        "pytorch_Test = torchvision.transforms.Compose([\n",
        "  torchvision.transforms.ToPILImage(),\n",
        "  torchvision.transforms.Resize((int(299), int(299))),\n",
        "  torchvision.transforms.ToTensor()]\n",
        ")\n",
        "\n",
        "test_ds = dataloader(test_data, transforms=pytorch_Test)\n",
        "test_loader = torch.utils.data.DataLoader(test_ds, num_workers=4)\n",
        "\n",
        "model_filename = model_path + '/best_model_vit.pth'\n",
        "model = mod.ViT_Pretrained()\n",
        "model.to(device)\n",
        "model.load_state_dict(torch.load(model_filename))\n",
        "model.eval()\n",
        "\n",
        "y_pred_trans = Compose([EnsureType(), Activations(sigmoid=True)])\n",
        "\n",
        "count = 0\n",
        "Image_FN_LIST = []\n",
        "Y_PRED_NP = np.zeros([len(test_data), num_classes])\n",
        "y_predicted = []\n",
        "y_true = []\n",
        "y_test = []\n",
        "y_score = []\n",
        "GT_LIST = []\n",
        "\n",
        "pb = progressbar.ProgressBar(maxval=len(test_data)+1)\n",
        "\n",
        "pb.start()\n",
        "with torch.no_grad():\n",
        "  for _testImage in test_loader:\n",
        "    image = torch.as_tensor(_testImage[0]).to(device)\n",
        "    y_test.append(np.array(_testImage[1]))\n",
        "    gt = _testImage[1].cpu().detach().numpy()\n",
        "    GT_LIST.append(int(np.argmax(gt[0])))\n",
        "    y_true.append(int(np.argmax(gt[0])))\n",
        "    y = model(image)\n",
        "    y_pred = y_pred_trans(y).to('cpu')\n",
        "    y_predicted.append(torch.argmax(y_pred).item())\n",
        "    pb.update(count)\n",
        "    count += 1 \n",
        "\n",
        "y_test = np.array([t.ravel() for t in y_test])\n",
        "y_score = np.array([t.ravel() for t in Y_PRED_NP]) \n",
        "\n",
        "print(classification_report(y_true, y_predicted))"
      ],
      "metadata": {
        "id": "vj04XIei2Z4p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}